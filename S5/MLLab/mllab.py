# -*- coding: utf-8 -*-
"""MLlab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AHLn2NAaWqp00sr7MuQiBwDb4fO6_Kex

Linear Regression [Base for all Regression Code]
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt

# Load CSV
data = pd.read_csv("/content/sample_data/california_housing_test.csv")

X = data[['housing_median_age']]
y = data['median_house_value']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

#Printing the Model
intercept = model.intercept_
coefficient = model.coef_[0]
print(f"Regression Equation: y = {intercept:.2f} + {coefficient:.2f} * x")

# Accuracy & R²
print("Accuracy (model.score):", model.score(X_test, y_test))
print("R² Score:", r2_score(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))

# Visualization with best-fit line
plt.scatter(X_test, y_test, color="blue", label="Actual")
plt.plot(X_test, y_pred, color="red", linewidth=2, label="Best Fit Line")
plt.xlabel("cname")
plt.ylabel("Target")
plt.legend()
plt.show()

"""Polynomial

added import


```
from sklearn.preprocessing import PolynomialFeatures
```



added This before test train split

```
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
```

Sorting For smooth visualization


```
X_seq = np.linspace(X.min(), X.max(), 300).reshape(-1,1)
X_seq_poly = poly.transform(X_seq)
y_seq = model.predict(X_seq_poly)
```




"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import numpy as np

# Load CSV
data = pd.read_csv("/content/sample_data/california_housing_test.csv")

X = data[['housing_median_age']]
y = data['median_house_value']

# Polynomial transformation (degree=2 for example)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

#print The coefs and Intercept
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# Accuracy & R²
print("Accuracy (model.score):", model.score(X_test, y_test))
print("R² Score:", r2_score(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))

# Visualization (sort X for smooth curve)
X_seq = np.linspace(X.min(), X.max(), 300).reshape(-1,1)
X_seq_poly = poly.transform(X_seq)
y_seq = model.predict(X_seq_poly)

plt.scatter(X, y, color="blue", alpha=0.5, label="Actual")
plt.plot(X_seq, y_seq, color="red", linewidth=2, label="Polynomial Fit")
plt.xlabel("housing_median_age")
plt.ylabel("median_house_value")
plt.legend()
plt.show()

"""Multivariable Linear Regression

import Added Numpy

changed X from 1 variable to 2


```
X = data[['housing_median_age', 'total_rooms']]
```

Visualization  Added lines (Actual Vs Predicted Visualization Only)


```
line = np.linspace(min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max()), 100)
plt.plot(line, line, color="red", linewidth=2, label="Best Fit Line")
```



"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error
import matplotlib.pyplot as plt
import numpy as np


# Load CSV
data = pd.read_csv("/content/sample_data/california_housing_test.csv")

# Use two features instead of one
X = data[['housing_median_age', 'total_rooms']]
y = data['median_house_value']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train model
model = LinearRegression()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

#print The coefs and Intercept
print("Coefficients:", model.coef_)
print("Intercept:", model.intercept_)

# Accuracy & R²
print("Accuracy (model.score):", model.score(X_test, y_test))
print("R² Score:", r2_score(y_test, y_pred))
print("MSE:", mean_squared_error(y_test, y_pred))

# Visualization
plt.scatter(y_test, y_pred, color="blue", alpha=0.5)
line = np.linspace(min(y_test.min(), y_pred.min()), max(y_test.max(), y_pred.max()), 100)
plt.plot(line, line, color="red", linewidth=2, label="Best Fit Line")
plt.xlabel("Actual median_house_value")
plt.ylabel("Predicted median_house_value")
plt.title("Multivariable Linear Regression")
plt.show()

"""Logistic Regression [Base for all Classifiation Code]"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Load CSV
data = pd.read_csv("/content/sample_data/california_housing_test.csv")

# Features
X = data[['housing_median_age', 'total_rooms']]

# Convert continuous target to binary (1 if above median, else 0)
y = (data['median_house_value'] > data['median_house_value'].median()).astype(int)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Logistic Regression
model = LogisticRegression(max_iter=1000)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Visualization (confusion matrix heatmap)
sns.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt="d", cmap="Blues")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix - Logistic Regression")
plt.show()

"""Naive Bayes Classifier

Changed The import from Logistic Regression to Gaussian NB


```
from sklearn.naive_bayes import GaussianNB
```

Changed the model to Gaussian NB


```
model = GaussianNB()
```




"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix,ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load CSV
data = pd.read_csv("/content/sample_data/california_housing_test.csv")

# Features
X = data[['housing_median_age', 'total_rooms']]

# Binary target (1 if above median, else 0)
y = (data['median_house_value'] > data['median_house_value'].median()).astype(int)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Naive Bayes
model = GaussianNB()
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

# Visualization (confusion matrix ConfusioMatrixDisplay)
disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred),
                              display_labels=['Below Median','Above Median'])
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - ID3 (Decision Tree)")
plt.show()

"""ID3

Changed The import to DesicionTreeClassifier


```
from sklearn.tree import DecisionTreeClassifier, plot_tree
```



Changed Model to DecisionTreeClassifier



```
model = DecisionTreeClassifier(criterion="entropy", random_state=42)
```



Added Decision Tree Visualization


```
plt.figure(figsize=(12,6))
plot_tree(model, feature_names=['housing_median_age', 'total_rooms'],
          class_names=['Below Median','Above Median'], filled=True)
plt.title("Decision Tree (ID3)")
plt.show()
```


"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# Load CSV
data = pd.read_csv("/content/sample_data/california_housing_test.csv")

# Features
X = data[['housing_median_age', 'total_rooms']]

# Binary target (1 if above median, else 0)
y = (data['median_house_value'] > data['median_house_value'].median()).astype(int)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Decision Tree using entropy (ID3-like)
model = DecisionTreeClassifier(criterion="entropy", random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)

# Accuracy
print("Accuracy:", accuracy_score(y_test, y_pred))
print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

#------ Optional -----

# Visualization (confusion matrix)
disp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(y_test, y_pred),
                              display_labels=['Below Median','Above Median'])
disp.plot(cmap="Blues")
plt.title("Confusion Matrix - ID3 (Decision Tree)")
plt.show()



# Visualization (decision tree)
plt.figure(figsize=(12,6))
plot_tree(model, feature_names=['housing_median_age', 'total_rooms'],
          class_names=['Below Median','Above Median'], filled=True)
plt.title("Decision Tree (ID3)")
plt.show()

